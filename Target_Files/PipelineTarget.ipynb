{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce2c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a833430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ NORMALIZED DATA FOR TARGET VERSION OF PIPELINE\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\normalizedResult.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e8d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ DATA THAT CONTAINS UNIQUE WORDS FOR EACH TARGET\n",
    "onlyPerson=[]\n",
    "onlyGroup=[]\n",
    "onlyPublic=[]\n",
    "\n",
    "personFile = open('onlyPersonWords.txt', 'r', encoding=\"utf-8\")\n",
    "temp = personFile.readlines()\n",
    "for line in temp:\n",
    "    onlyPerson.append(line.replace(\"\\n\", \"\"))\n",
    "    \n",
    "groupFile = open('onlyGroupWords.txt', 'r', encoding=\"utf-8\")\n",
    "temp = groupFile.readlines()\n",
    "for line in temp:\n",
    "    onlyGroup.append(line.replace(\"\\n\", \"\"))\n",
    "\n",
    "publicFile = open('onlyPublicWords.txt', 'r', encoding=\"utf-8\")\n",
    "temp = publicFile.readlines()\n",
    "for line in temp:\n",
    "    onlyPublic.append(line.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189c58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8132\n",
      "8132\n"
     ]
    }
   ],
   "source": [
    "#CREATE TWO LISTS THAT CONTAIN EACH TEXT OF A TWEET AND ITS TARGET\n",
    "target = (df.iloc[0:8132 , 2 ].to_list())\n",
    "text= df.iloc[0:8132, 1 ].to_list()\n",
    "print (len(text))\n",
    "print (len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7ad1e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39068\n",
      "39068\n"
     ]
    }
   ],
   "source": [
    "#ADDING ADDITIONAL WORDS WITH CORRESPONDING TARGETS\n",
    "for x in range(0, 2):\n",
    "    for word in onlyPerson:\n",
    "        text.append(word)\n",
    "        target.append(0)\n",
    "    for word in onlyGroup:\n",
    "        text.append(word)\n",
    "        target.append(1)\n",
    "    for word in onlyPublic:\n",
    "        text.append(word)\n",
    "        target.append(2)\n",
    "print (len(text))\n",
    "print (len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6dadd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39017\n",
      "39017\n"
     ]
    }
   ],
   "source": [
    "#REMOVAL OF TWEETS WITHOUT ANY CONTENT AND ITS CORRESPONDING ENTRY IN TARGET LIST\n",
    "counter=0\n",
    "for t in text:\n",
    "    if type(t) is float:\n",
    "        text.remove(t)\n",
    "        target.remove(target[counter])\n",
    "    counter+=1\n",
    "\n",
    "print (len(text))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d33eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENISATION OF TWEETS AND LOWERCASING OF WORDS\n",
    "array=[]\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "for t in text:\n",
    "    doc = nlp(t)\n",
    "\n",
    "    temp = [token.lemma_.lower() for token in doc if token.is_alpha \n",
    "#             and not token.pos_ == 'ADJ' \n",
    "#             and not token.pos_ == 'CCONJ'\n",
    "#             and not token.pos_ == 'SCONJ' \n",
    "#             and not token.pos_ == 'INTJ'\n",
    "#             and not token.pos_ == 'PREP'\n",
    "#             and not token.pos_ == 'ADV'\n",
    "#             and not token.pos_ == 'VERB'\n",
    "#               and token.pos_ == \"AUX\"\n",
    "#               or token.pos_ == \"DET\"\n",
    "#               or token.pos_ == \"NOUN\"\n",
    "#               or token.pos_ == \"PRON\"\n",
    "#               or token.pos_ == \"PROPN\"\n",
    "#               or token.pos_ == \"VERB\"\n",
    "#               or token.pos_ == \"ADV\"\n",
    "              ]\n",
    "    array.append((\" \".join(temp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48183cb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RANDOMIZER THAT ZIPS TWO CORRESPONDING LISTS TOGETHER AND SHUFFLES BOTH LISTS SYMMETRICALLY\n",
    "# temporary = list(zip(text, target))\n",
    "# random.shuffle(temporary)\n",
    "# text, target = zip(*temporary)\n",
    "# text, target = list(text), list(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590e677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME THAT CONTAINS TWEET TEXT AND TARGET\n",
    "df = pd.DataFrame ({'Text': array , 'Target': target })\n",
    "#print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90096686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  (31213,)\n",
      "Shape of X_test:  (7804,)\n"
     ]
    }
   ],
   "source": [
    "#SPLIT DATA INTO TRAIN AND TEST DATA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Text, \n",
    "    df.Target, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022, #-> same order of dataset\n",
    "    stratify=df.Target\n",
    ")\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc6e294c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12674\n",
       "0    10949\n",
       "2     7590\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "825a61f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3169\n",
       "0    2737\n",
       "2    1898\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c976339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.80      0.77      2737\n",
      "           1       0.74      0.86      0.79      3169\n",
      "           2       0.93      0.60      0.73      1898\n",
      "\n",
      "    accuracy                           0.77      7804\n",
      "   macro avg       0.81      0.75      0.77      7804\n",
      "weighted avg       0.79      0.77      0.77      7804\n",
      "\n",
      "The accuracy of prediction is:  0.774218349564326\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     #('KNN', KNeighborsClassifier())\n",
    "     ('SVM', SVC()),\n",
    "     #('rfc', RandomForestClassifier()) \n",
    "     #('xgb', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)) \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# find accuracy scores\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy of prediction is: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e5131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[:100]\n",
    "# c =0\n",
    "# for i in X_test[:200]:\n",
    "#     print(c,\"->\", i)\n",
    "#     c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12aa1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of target Person:  2737\n",
      "count of target Group:  3169\n",
      "count of target Public 1898\n"
     ]
    }
   ],
   "source": [
    "#y_test[0:50]\n",
    "# c =0\n",
    "# for i in y_test[0:]:\n",
    "#     print(c,\"->\", i)\n",
    "#     c+=1\n",
    "\n",
    "person=0\n",
    "group=0\n",
    "public=0\n",
    "for i in y_test:\n",
    "    if i == 2:\n",
    "        public+=1\n",
    "    if i == 1:\n",
    "        group+=1\n",
    "    if i == 0:\n",
    "        person+=1\n",
    "print(\"count of target Person: \", person) \n",
    "print(\"count of target Group: \", group)\n",
    "print(\"count of target Public\", public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f842cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of target Person:  2902\n",
      "count of target Group:  3669\n",
      "count of target Public 1233\n"
     ]
    }
   ],
   "source": [
    "#y_pred[0:50]\n",
    "# c =0\n",
    "# for i in y_pred[0:]:\n",
    "#     print(c,\"->\", i)\n",
    "#     c+=1\n",
    "\n",
    "person=0\n",
    "group=0\n",
    "public=0\n",
    "for i in y_pred:\n",
    "    if i == 2:\n",
    "        public+=1\n",
    "    if i == 1:\n",
    "        #print(i)\n",
    "        group+=1\n",
    "    if i == 0:\n",
    "        #print(i)\n",
    "        person+=1\n",
    "print(\"count of target Person: \", person) \n",
    "print(\"count of target Group: \", group)\n",
    "print(\"count of target Public\", public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fbdd3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
