{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ce2c7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a833430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ NORMALIZED DATA FOR TARGET VERSION OF PIPELINE\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\normalizedResult.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ca76832",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyPersonTweets=[]\n",
    "onlyGroupTweets=[]\n",
    "onlyPublicTweets=[]\n",
    "\n",
    "personFile = open('targetPersonTweets.txt', 'r', encoding=\"utf-8\")\n",
    "temp = personFile.readlines()\n",
    "for line in temp:\n",
    "    onlyPersonTweets.append(line.replace(\" \\n\", \"\"))\n",
    "    \n",
    "groupFile = open('targetGroupTweets.txt', 'r', encoding=\"utf-8\")\n",
    "temp = groupFile.readlines()\n",
    "for line in temp:\n",
    "    onlyGroupTweets.append(line.replace(\" \\n\", \"\"))\n",
    "\n",
    "publicFile = open('targetPublicTweets.txt', 'r', encoding=\"utf-8\")\n",
    "temp = publicFile.readlines()\n",
    "for line in temp:\n",
    "    onlyPublicTweets.append(line.replace(\" \\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189c58de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8081\n",
      "8081\n"
     ]
    }
   ],
   "source": [
    "#CREATE TWO LISTS THAT CONTAIN EACH TEXT OF A TWEET AND ITS TARGET\n",
    "df=df.dropna()\n",
    "target=(df.iloc[0:8132 , 2 ].to_list())\n",
    "text=df.iloc[0:8132, 1 ].to_list()\n",
    "print(len(text))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "590e677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME THAT CONTAINS TWEET TEXT AND TARGET\n",
    "df = pd.DataFrame ({'Text': text , 'Target': target })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90096686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  6464\n",
      "Shape of X_test:  1617\n"
     ]
    }
   ],
   "source": [
    "#SPLIT DATA INTO TRAIN AND TEST DATA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    text, \n",
    "    target, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2022, #-> same order of dataset\n",
    "    stratify=target\n",
    ")\n",
    "print(\"Shape of X_train: \", len(X_train))\n",
    "print(\"Shape of X_test: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c0f445",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TARGET SPLITTING OF 0.8 DATA###\n",
    "targetPerson=[]\n",
    "targetGroup=[]\n",
    "targetPublic=[]\n",
    "\n",
    "counter=0\n",
    "for t in X_train:\n",
    "    if y_train[counter] == 0:\n",
    "        targetPerson.append(t)\n",
    "    elif y_train[counter] == 1:\n",
    "        targetGroup.append(t)\n",
    "    else:\n",
    "        targetPublic.append(t)\n",
    "    counter+=1\n",
    "###TARGET SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2724181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###UNIQUE WORD SPLITTING OF 0.8 DATA###\n",
    "personWords=[]\n",
    "groupWords=[]\n",
    "publicWords=[]\n",
    "\n",
    "for tweet in targetPerson:\n",
    "    personWords.append(tweet.split())\n",
    "personWords = sum(personWords, [])\n",
    "\n",
    "for tweet in targetGroup:\n",
    "    groupWords.append(tweet.split())\n",
    "groupWords = sum(groupWords, [])\n",
    "\n",
    "for tweet in targetPublic:\n",
    "    publicWords.append(tweet.split())\n",
    "publicWords = sum(publicWords, [])\n",
    "\n",
    "personUnique=[]\n",
    "groupUnique=[]\n",
    "publicUnique=[]\n",
    "\n",
    "for word in personWords:\n",
    "    if word not in personUnique:\n",
    "        personUnique.append(word)\n",
    "        \n",
    "for word in groupWords:\n",
    "    if word not in groupUnique:\n",
    "        groupUnique.append(word)\n",
    "        \n",
    "for word in publicWords:\n",
    "    if word not in publicUnique:\n",
    "        publicUnique.append(word)\n",
    "        \n",
    "onlyPerson=[]\n",
    "\n",
    "for word in personUnique:\n",
    "    if word not in groupUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyPerson.append(word)\n",
    "            \n",
    "onlyGroup=[]\n",
    "\n",
    "for word in groupUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyGroup.append(word)\n",
    "            \n",
    "onlyPublic=[]\n",
    "\n",
    "for word in publicUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in groupUnique:\n",
    "            onlyPublic.append(word)\n",
    "###WORD SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c976339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person: 563\n",
      "Group: 670\n",
      "Public: 384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     #('KNN', KNeighborsClassifier())\n",
    "     #('SVM', SVC()),\n",
    "     #('rfc', RandomForestClassifier()) \n",
    "     ('xgb', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)) \n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#4. print the classfication report BEFORE ANALYZER\n",
    "classificationBefore = classification_report(y_test, y_pred)\n",
    "\n",
    "#5. find accuracy scores\n",
    "accuracyBefore = accuracy_score(y_test, y_pred)\n",
    "\n",
    "personIst=0\n",
    "groupIst=0\n",
    "publicIst=0\n",
    "for p in y_pred:\n",
    "    if p == 0:\n",
    "        personIst+=1\n",
    "    if p == 1:\n",
    "        groupIst+=1\n",
    "    if p == 2:\n",
    "        publicIst+=1\n",
    "print(\"Person: \" + str(personIst))\n",
    "print(\"Group: \" + str(groupIst))\n",
    "print(\"Public: \" + str(publicIst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69807dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. run analyzer for target correction\n",
    "y_pred2=y_pred\n",
    "countList=[]\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "counter=0\n",
    "single_pronouns=[\"du\", \"dich\", \"deinen\"]\n",
    "single_adress=[\"herr\", \"herrn\", \"frau\", \"junge\", \"nazi\"]\n",
    "single_politicians=[\"merkel\", \"söder\", \"spahn\", \"soeder\", \"laschet\", \"baerbock\", \"bundeskanzlerin\", \"bundeskanzler\", \"kanzler\", \"kanzlerin\"]\n",
    "group_pronouns=[\"euch\", \"eure\", \"euer\", \"deren\", \"ihr\"]\n",
    "group_politics=[\"spd\", \"cdu\", \"csu\", \"afd\", \"grünen\", \"grüne\", \"union\", \"linken\", \"nazis\", \"rechten\", \"partei\", \"land\", \"terroristen\", \"demokratie\"]\n",
    "group_countries=[\"deutschland\", \"deutsche\", \"bürger\"]\n",
    "public_pronouns=[\"alle\", \"wir\", \"menschen\", \"volk\", \"bevölkerung\", \"welt\", \"bewohner\"]\n",
    "testList=public_pronouns\n",
    "\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 1:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1.2\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1.2\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=0.9\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries:\n",
    "                groupCount+=0.9\n",
    "        if personCount > groupCount:\n",
    "            y_pred2[counter]=0\n",
    "        personCount=0\n",
    "        groupCount=0\n",
    "    counter+=1\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "counter=0\n",
    "\n",
    "personIst2=0\n",
    "groupIst2=0\n",
    "publicIst2=0\n",
    "for p in y_pred2:\n",
    "    if p == 0:\n",
    "        personIst2+=1\n",
    "    if p == 1:\n",
    "        groupIst2+=1\n",
    "    if p == 2:\n",
    "        publicIst2+=1\n",
    "\n",
    "#7. print the classfication report AFTER ANALYZER\n",
    "classificationAfter = classification_report(y_test, y_pred2)\n",
    "\n",
    "#8. find accuracy scores\n",
    "accuracyAfter = accuracy_score(y_test, y_pred2)\n",
    "    \n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 0:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=0.5\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries:\n",
    "                groupCount+=0.5\n",
    "        if groupCount > personCount:\n",
    "            y_pred2[counter]=1\n",
    "        personCount=0\n",
    "        groupCount=0\n",
    "    counter+=1\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "counter=0\n",
    "\n",
    "personIst3=0\n",
    "groupIst3=0\n",
    "publicIst3=0\n",
    "for p in y_pred2:\n",
    "    if p == 0:\n",
    "        personIst3+=1\n",
    "    if p == 1:\n",
    "        groupIst3+=1\n",
    "    if p == 2:\n",
    "        publicIst3+=1\n",
    "\n",
    "#7. print the classfication report AFTER ANALYZER\n",
    "classificationAfter2 = classification_report(y_test, y_pred2)\n",
    "\n",
    "#8. find accuracy scores\n",
    "accuracyAfter2 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 0 or y_pred2[counter] == 1:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=1\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries:\n",
    "                groupCount+=1\n",
    "            if token in onlyPublic:\n",
    "                publicCount+=1\n",
    "            if token in public_pronouns:\n",
    "                publicCount+=1\n",
    "        if publicCount > personCount and publicCount > groupCount:\n",
    "            y_pred2[counter]=2\n",
    "        personCount=0\n",
    "        groupCount=0\n",
    "        publicCount=0\n",
    "    counter+=1\n",
    "\n",
    "personIst4=0\n",
    "groupIst4=0\n",
    "publicIst4=0\n",
    "for p in y_pred2:\n",
    "    if p == 0:\n",
    "        personIst4+=1\n",
    "    if p == 1:\n",
    "        groupIst4+=1\n",
    "    if p == 2:\n",
    "        publicIst4+=1\n",
    "\n",
    "#7. print the classfication report AFTER ANALYZER\n",
    "classificationAfter3 = classification_report(y_test, y_pred2)\n",
    "\n",
    "#8. find accuracy scores\n",
    "accuracyAfter3 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "###################################################################\n",
    "# print(testList)\n",
    "# freq=[]\n",
    "# for i in testList:\n",
    "#     freq.append(0)\n",
    "# for p in onlyPersonTweets:\n",
    "#     doc=onlyPersonTweets[counter].split()\n",
    "#     for token in doc:\n",
    "#         if token.lower() in testList:\n",
    "#             c=0\n",
    "#             for w in testList:\n",
    "#                 if token.lower() == w:\n",
    "#                     freq[c]+=1\n",
    "#                 c+=1\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "# print(freq)\n",
    "# freq.clear()\n",
    "\n",
    "# for i in testList:\n",
    "#     freq.append(0)\n",
    "# for p in onlyGroupTweets:\n",
    "#     doc=onlyGroupTweets[counter].split()\n",
    "#     for token in doc:\n",
    "#         if token.lower() in testList:\n",
    "#             c=0\n",
    "#             for w in testList:\n",
    "#                 if token.lower() == w:\n",
    "#                     freq[c]+=1\n",
    "#                 c+=1\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "# print(freq)\n",
    "# freq.clear() \n",
    "\n",
    "# for i in testList:\n",
    "#     freq.append(0)\n",
    "# for p in onlyPublicTweets:\n",
    "#     doc=onlyPublicTweets[counter].split()\n",
    "#     for token in doc:\n",
    "#         if token.lower() in testList:\n",
    "#             c=0\n",
    "#             for w in testList:\n",
    "#                 if token.lower() == w:\n",
    "#                     freq[c]+=1\n",
    "#                 c+=1\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "# print(freq)\n",
    "# freq.clear() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "665779a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE ANALZYER:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.66      0.67       571\n",
      "           1       0.65      0.68      0.66       642\n",
      "           2       0.51      0.49      0.50       404\n",
      "\n",
      "    accuracy                           0.62      1617\n",
      "   macro avg       0.61      0.61      0.61      1617\n",
      "weighted avg       0.62      0.62      0.62      1617\n",
      "\n",
      "The accuracy of prediction is:  0.6239950525664811\n",
      "IST-WERT DAVOR:\n",
      "Person: 563\n",
      "Group: 670\n",
      "Public: 384\n",
      "###############################################\n",
      "AFTER ANALYZER 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.75      0.65       571\n",
      "           1       0.69      0.53      0.60       642\n",
      "           2       0.51      0.49      0.50       404\n",
      "\n",
      "    accuracy                           0.59      1617\n",
      "   macro avg       0.59      0.59      0.58      1617\n",
      "weighted avg       0.60      0.59      0.59      1617\n",
      "\n",
      "The accuracy of prediction is:  0.5949288806431664\n",
      "IST-WERT nach Durchlauf 1:\n",
      "Person: 738\n",
      "Group: 495\n",
      "Public: 384\n",
      "AFTER ANALYZER 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.68      0.64       571\n",
      "           1       0.64      0.58      0.61       642\n",
      "           2       0.51      0.49      0.50       404\n",
      "\n",
      "    accuracy                           0.59      1617\n",
      "   macro avg       0.58      0.58      0.58      1617\n",
      "weighted avg       0.59      0.59      0.59      1617\n",
      "\n",
      "The accuracy of prediction is:  0.5924551638837353\n",
      "IST-WERT nach Durchlauf 2:\n",
      "Person: 652\n",
      "Group: 581\n",
      "Public: 384\n",
      "AFTER ANALYZER 3:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.63      0.62       571\n",
      "           1       0.64      0.50      0.57       642\n",
      "           2       0.44      0.57      0.50       404\n",
      "\n",
      "    accuracy                           0.57      1617\n",
      "   macro avg       0.57      0.57      0.56      1617\n",
      "weighted avg       0.58      0.57      0.57      1617\n",
      "\n",
      "The accuracy of prediction is:  0.5664811379097093\n",
      "IST-WERT nach Durchlauf 3:\n",
      "Person: 595\n",
      "Group: 501\n",
      "Public: 521\n"
     ]
    }
   ],
   "source": [
    "print(\"BEFORE ANALZYER:\")\n",
    "print(classificationBefore)\n",
    "print(\"The accuracy of prediction is: \", accuracyBefore)\n",
    "print(\"IST-WERT DAVOR:\")\n",
    "print(\"Person: \" + str(personIst))\n",
    "print(\"Group: \" + str(groupIst))\n",
    "print(\"Public: \" + str(publicIst))\n",
    "print(\"###############################################\")\n",
    "print(\"AFTER ANALYZER 1:\")\n",
    "print(classificationAfter)\n",
    "print(\"The accuracy of prediction is: \", accuracyAfter)\n",
    "print(\"IST-WERT nach Durchlauf 1:\")\n",
    "print(\"Person: \" + str(personIst2))\n",
    "print(\"Group: \" + str(groupIst2))\n",
    "print(\"Public: \" + str(publicIst2))\n",
    "print(\"AFTER ANALYZER 2:\")\n",
    "print(classificationAfter2)\n",
    "print(\"The accuracy of prediction is: \", accuracyAfter2)\n",
    "print(\"IST-WERT nach Durchlauf 2:\")\n",
    "print(\"Person: \" + str(personIst3))\n",
    "print(\"Group: \" + str(groupIst3))\n",
    "print(\"Public: \" + str(publicIst3))\n",
    "print(\"AFTER ANALYZER 3:\")\n",
    "print(classificationAfter3)\n",
    "print(\"The accuracy of prediction is: \", accuracyAfter3)\n",
    "print(\"IST-WERT nach Durchlauf 3:\")\n",
    "print(\"Person: \" + str(personIst4))\n",
    "print(\"Group: \" + str(groupIst4))\n",
    "print(\"Public: \" + str(publicIst4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
