{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00af3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2551d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# emb1 = model.encode(\"Test\")\n",
    "# emb2 = model.encode(\"Test\")\n",
    "\n",
    "# cos_sim = util.cos_sim(emb1, emb2)\n",
    "# print(\"Cosine-Similarity:\", cos_sim)\n",
    "# print(cos_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ NORMALIZED DATA FOR TARGET VERSION OF PIPELINE\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\normalizedResult.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243362f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TWO LISTS THAT CONTAIN EACH TEXT OF A TWEET AND ITS TARGET\n",
    "df=df.dropna()\n",
    "target=(df.iloc[0:8132 , 2 ].to_list())\n",
    "text=df.iloc[0:8132, 1 ].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d75b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME THAT CONTAINS TWEET TEXT AND TARGET\n",
    "df = pd.DataFrame ({'Text': text , 'Target': target })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5164e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Text, \n",
    "    df.Target, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2021,#-> same order of dataset random_state=2022, 2021\n",
    "    stratify=df.Target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be911ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TARGET SPLITTING OF 0.8 DATA###\n",
    "targetPerson=[]\n",
    "targetGroup=[]\n",
    "targetPublic=[]\n",
    "\n",
    "X_trainList = X_train.tolist()\n",
    "y_trainList = y_train.tolist()\n",
    "\n",
    "counter=0\n",
    "for t in X_trainList:\n",
    "    if y_trainList[counter] == 0:\n",
    "        targetPerson.append(t)\n",
    "    elif y_trainList[counter] == 1:\n",
    "        targetGroup.append(t)\n",
    "    else:\n",
    "        targetPublic.append(t)\n",
    "    counter+=1\n",
    "###TARGET SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99250b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "###UNIQUE WORD SPLITTING OF 0.8 DATA###\n",
    "personWords=[]\n",
    "groupWords=[]\n",
    "publicWords=[]\n",
    "\n",
    "for tweet in targetPerson:\n",
    "    personWords.append(tweet.split())\n",
    "personWords = sum(personWords, [])\n",
    "\n",
    "for tweet in targetGroup:\n",
    "    groupWords.append(tweet.split())\n",
    "groupWords = sum(groupWords, [])\n",
    "\n",
    "for tweet in targetPublic:\n",
    "    publicWords.append(tweet.split())\n",
    "publicWords = sum(publicWords, [])\n",
    "\n",
    "personUnique=[]\n",
    "groupUnique=[]\n",
    "publicUnique=[]\n",
    "\n",
    "for word in personWords:\n",
    "    if word not in personUnique:\n",
    "        personUnique.append(word)\n",
    "        \n",
    "for word in groupWords:\n",
    "    if word not in groupUnique:\n",
    "        groupUnique.append(word)\n",
    "        \n",
    "for word in publicWords:\n",
    "    if word not in publicUnique:\n",
    "        publicUnique.append(word)\n",
    "        \n",
    "onlyPerson=[]\n",
    "\n",
    "for word in personUnique:\n",
    "    if word not in groupUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyPerson.append(word)\n",
    "            \n",
    "onlyGroup=[]\n",
    "\n",
    "for word in groupUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyGroup.append(word)\n",
    "            \n",
    "onlyPublic=[]\n",
    "\n",
    "for word in publicUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in groupUnique:\n",
    "            onlyPublic.append(word)\n",
    "###UNIQUE WORD SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf00879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6464, 20470)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_classification \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tfidf\n",
    "#vecktorisieren\n",
    "\n",
    "vectorizer = TfidfVectorizer() \n",
    "Xtrain = vectorizer.fit_transform(X_train) \n",
    "Xtest = vectorizer.transform(X_test) \n",
    "print(Xtrain.shape)\n",
    "\n",
    "#vecktorisieren \n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('all-mpnet-base-v2')\n",
    "#model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "#model = SentenceTransformer('all-distilroberta-v1') multi-qa-distilbert-cos-v1\n",
    "#model = SentenceTransformer('multi-qa-distilbert-cos-v1') #multi-qa-MiniLM-L6-cos-v1\n",
    "#model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1') #paraphrase-multilingual-mpnet-base-v2\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') # best #paraphrase-albert-small-v2\n",
    "#model = SentenceTransformer('paraphrase-albert-small-v2') #paraphrase-multilingual-MiniLM-L12-v2\n",
    "#model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') #paraphrase-MiniLM-L3-v2\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L3-v2') #distiluse-base-multilingual-cased-v1\n",
    "#odel = SentenceTransformer('distiluse-base-multilingual-cased-v1') #distiluse-base-multilingual-cased-v2\n",
    "#odel = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "#print(X_train)\n",
    "x = X_train.tolist()\n",
    "Xtrain = model.encode(x) \n",
    "xt = X_test.tolist()\n",
    "Xtest = model.encode(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acecb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create classifier and train model\n",
    "\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021, #booster='gblinear', 2021\n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 100\n",
    "                           n_estimators=100, #1000: 0.6357452071737786\n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 0.0\n",
    "                           gamma=0, #0.9 best with 0.6536796536796536\n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 1\n",
    "                           min_child_weight=1, #19 best with 0.6524427952999382 \n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 0.3\n",
    "                           learning_rate=0.3, #0.2 best with 0.6462585034013606\n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 6\n",
    "                           reg_alpha=6, #17 best with 0.6524427952999382\n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 6\n",
    "                           max_depth=6, #11 best with 0.6499690785405071\n",
    "\n",
    "                           #Best Acc: 0.6598639455782312 for x: 0.7 | y: 0.6 | z: 1\n",
    "                           colsample_bytree=0.7, colsample_bylevel=0.6, colsample_bynode=1 #0.7, 0.6, 1.0 best with 0.6586270871985158\n",
    "                       )\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(Xtrain, y_train)\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(Xtest)\n",
    "\n",
    "#4. classfication report BEFORE ANALYZER\n",
    "classificationBefore = classification_report(y_test, y_pred)\n",
    "\n",
    "#5. find accuracy scores\n",
    "accuracyBefore = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66fd0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. count prediction targets\n",
    "personIst=0\n",
    "groupIst=0\n",
    "publicIst=0\n",
    "for p in y_pred:\n",
    "    if p == 0:\n",
    "        personIst+=1\n",
    "    if p == 1:\n",
    "        groupIst+=1\n",
    "    if p == 2:\n",
    "        publicIst+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2794cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=y_pred\n",
    "countList=[]\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "counter=0\n",
    "single_pronouns=[\"du\", \"dich\", \"dein\", \"er\", \"sein\", \"mein\", \"meine\", \"mir\"]\n",
    "single_adress=[\"herr\", \"herrn\", \"frau\", \"junge\", \"nazi\"]\n",
    "single_politicians=[\"merkel\", \"söder\", \"spahn\", \"soeder\", \"laschet\", \"baerbock\", \"bundeskanzlerin\", \"bundeskanzler\", \"kanzler\", \"kanzlerin\", \"putin\", \"trump\", \"erdogan\"]\n",
    "group_pronouns=[\"euch\", \"eure\", \"euer\", \"deren\", \"ihr\", \"unsere\", \"uns\"]\n",
    "group_politics=[\"spd\", \"cdu\", \"csu\", \"afd\", \"grünen\", \"grüne\", \"union\", \"linken\", \"nazis\", \"rechten\", \"partei\", \"land\", \"terroristen\", \"demokratie\"]\n",
    "group_religion=[\"muslime\", \"islamische\", \"islam\", \"islamistische\", \"christen\", \"juden\"]\n",
    "group_countries=[\"deutschland\", \"deutsche\", \"bürger\", \"araber\", \"ausländer\", \"schwarzen\"]\n",
    "public_pronouns=[\"alle\", \"wir\", \"menschen\", \"volk\", \"bevölkerung\", \"welt\"]\n",
    "testList=public_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec113793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onlyPersonTweets=[]\n",
    "# onlyGroupTweets=[]\n",
    "# onlyPublicTweets=[]\n",
    "\n",
    "# personFile = open('targetPersonTweets.txt', 'r', encoding=\"utf-8\")\n",
    "# temp = personFile.readlines()\n",
    "# for line in temp:\n",
    "#     onlyPersonTweets.append(str(line.replace(\" \\n\", \"\")).lower())\n",
    "    \n",
    "# groupFile = open('targetGroupTweets.txt', 'r', encoding=\"utf-8\")\n",
    "# temp = groupFile.readlines()\n",
    "# for line in temp:\n",
    "#     onlyGroupTweets.append(str(line.replace(\" \\n\", \"\")).lower())\n",
    "\n",
    "# publicFile = open('targetPublicTweets.txt', 'r', encoding=\"utf-8\")\n",
    "# temp = publicFile.readlines()\n",
    "# for line in temp:\n",
    "#     onlyPublicTweets.append(str(line.replace(\" \\n\", \"\")).lower())\n",
    "\n",
    "# print(testList)\n",
    "# freq=[]\n",
    "# for i in testList:\n",
    "#     freq.append(0)\n",
    "# for p in onlyPersonTweets:\n",
    "#     doc=onlyPersonTweets[counter].split()\n",
    "#     for token in doc:\n",
    "#         if token.lower() in testList:\n",
    "#             c=0\n",
    "#             for w in testList:\n",
    "#                 if token.lower() == w:\n",
    "#                     freq[c]+=1\n",
    "#                 c+=1\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "# print(freq)\n",
    "# freq.clear()\n",
    "\n",
    "# for i in testList:\n",
    "#     freq.append(0)\n",
    "# for p in onlyGroupTweets:\n",
    "#     doc=onlyGroupTweets[counter].split()\n",
    "#     for token in doc:\n",
    "#         if token.lower() in testList:\n",
    "#             c=0\n",
    "#             for w in testList:\n",
    "#                 if token.lower() == w:\n",
    "#                     freq[c]+=1\n",
    "#                 c+=1\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "# print(freq)\n",
    "# freq.clear() \n",
    "\n",
    "# for i in testList:\n",
    "#     freq.append(0)\n",
    "# for p in onlyPublicTweets:\n",
    "#     doc=onlyPublicTweets[counter].split()\n",
    "#     for token in doc:\n",
    "#         if token.lower() in testList:\n",
    "#             c=0\n",
    "#             for w in testList:\n",
    "#                 if token.lower() == w:\n",
    "#                     freq[c]+=1\n",
    "#                 c+=1\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "# print(freq)\n",
    "# freq.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "910902e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. run analyzer for target correction\n",
    "#7.1 filter out person tweets out of group tweets and correct prediction\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 1:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1\n",
    "        #7       (0.6239950525664811)\n",
    "        #6       (0.6246134817563389)\n",
    "        #5       (0.6246134817563389) <-- BEST\n",
    "        #4       (0.6246134817563389)\n",
    "        #3       (0.6239950525664811)\n",
    "        if personCount >= 5:\n",
    "            y_pred2[counter]=0\n",
    "        personCount=0\n",
    "    counter+=1\n",
    "counter=0\n",
    "\n",
    "#7.1.1. classfication report AFTER ANALYZER (Step 1/3)\n",
    "classificationAfter = classification_report(y_test, y_pred2)\n",
    "\n",
    "#7.1.2. find accuracy scores\n",
    "accuracyAfter = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "#7.1.3. count prediction targets\n",
    "personIst2=0\n",
    "groupIst2=0\n",
    "publicIst2=0\n",
    "for p in y_pred2:\n",
    "    if p == 0:\n",
    "        personIst2+=1\n",
    "    if p == 1:\n",
    "        groupIst2+=1\n",
    "    if p == 2:\n",
    "        publicIst2+=1\n",
    "\n",
    "#7.2. filter out group tweets out of person tweets and correct prediction\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 0:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=1\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries or token in group_religion:\n",
    "                groupCount+=1\n",
    "        #6       (0.6246134817563389)\n",
    "        #5       (0.6252319109461967) <-- BEST\n",
    "        #4       (0.6252319109461967)\n",
    "        #3       (0.6246134817563389)\n",
    "        #2       (0.6171923314780458)\n",
    "        if groupCount >= 5:\n",
    "            y_pred2[counter]=1\n",
    "        groupCount=0\n",
    "    counter+=1\n",
    "counter=0\n",
    "\n",
    "#7.2.1. classfication report AFTER ANALYZER (Step 2/3)\n",
    "classificationAfter2 = classification_report(y_test, y_pred2)\n",
    "\n",
    "#7.2.2. find accuracy scores\n",
    "accuracyAfter2 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "#7.2.3. count prediction targets\n",
    "personIst3=0\n",
    "groupIst3=0\n",
    "publicIst3=0\n",
    "for p in y_pred2:\n",
    "    if p == 0:\n",
    "        personIst3+=1\n",
    "    if p == 1:\n",
    "        groupIst3+=1\n",
    "    if p == 2:\n",
    "        publicIst3+=1\n",
    "\n",
    "#7.3. filter out public tweets out of person and group tweets and correct prediction\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 0 or y_pred2[counter] == 1:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPublic:\n",
    "                publicCount+=1\n",
    "            if token in public_pronouns:\n",
    "                publicCount+=1\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=1\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries or token in group_religion:\n",
    "                groupCount+=1\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1\n",
    "        #6       (0.6252319109461967)\n",
    "        #5       (0.6258503401360545) <-- BEST\n",
    "        #4       (0.6258503401360545)\n",
    "        #3       (0.6190476190476191)\n",
    "        if publicCount > groupCount - 3 or publicCount > personCount - 3:\n",
    "            y_pred2[counter]=2\n",
    "        publicCount=0\n",
    "    counter+=1\n",
    "counter=0\n",
    "\n",
    "#7.3.1. classfication report AFTER ANALYZER (Step 3/3)\n",
    "classificationAfter3 = classification_report(y_test, y_pred2)\n",
    "\n",
    "#7.3.2. find accuracy scores\n",
    "accuracyAfter3 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "#7.3.3. count prediction targets\n",
    "personIst4=0\n",
    "groupIst4=0\n",
    "publicIst4=0\n",
    "for p in y_pred2:\n",
    "    if p == 0:\n",
    "        personIst4+=1\n",
    "    if p == 1:\n",
    "        groupIst4+=1\n",
    "    if p == 2:\n",
    "        publicIst4+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e6e790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE ANALZYER:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70       571\n",
      "           1       0.66      0.73      0.69       642\n",
      "           2       0.60      0.49      0.54       404\n",
      "\n",
      "    accuracy                           0.66      1617\n",
      "   macro avg       0.65      0.64      0.64      1617\n",
      "weighted avg       0.66      0.66      0.66      1617\n",
      "\n",
      "The accuracy of prediction is:  0.6598639455782312\n",
      "IST-WERT BEFORE ANALYZER:\n",
      "Person: 577\n",
      "Group: 715\n",
      "Public: 325\n",
      "###############################################\n",
      "AFTER ANALYZER STEP 1:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70       571\n",
      "           1       0.66      0.74      0.70       642\n",
      "           2       0.60      0.49      0.54       404\n",
      "\n",
      "    accuracy                           0.66      1617\n",
      "   macro avg       0.65      0.64      0.64      1617\n",
      "weighted avg       0.66      0.66      0.66      1617\n",
      "\n",
      "The accuracy of prediction is:  0.6611008039579468\n",
      "IST-WERT AFTER STEP 1:\n",
      "Person: 577\n",
      "Group: 715\n",
      "Public: 325\n",
      "AFTER ANALYZER STEP 2:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       571\n",
      "           1       0.66      0.74      0.69       642\n",
      "           2       0.60      0.49      0.54       404\n",
      "\n",
      "    accuracy                           0.66      1617\n",
      "   macro avg       0.65      0.64      0.64      1617\n",
      "weighted avg       0.66      0.66      0.66      1617\n",
      "\n",
      "The accuracy of prediction is:  0.660482374768089\n",
      "IST-WERT AFTER STEP 2:\n",
      "Person: 574\n",
      "Group: 718\n",
      "Public: 325\n",
      "AFTER ANALYZER\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70       571\n",
      "           1       0.66      0.74      0.69       642\n",
      "           2       0.60      0.49      0.54       404\n",
      "\n",
      "    accuracy                           0.66      1617\n",
      "   macro avg       0.65      0.64      0.64      1617\n",
      "weighted avg       0.66      0.66      0.66      1617\n",
      "\n",
      "The accuracy of prediction is:  0.6611008039579468\n",
      "IST-WERT AFTER\n",
      "Person: 572\n",
      "Group: 717\n",
      "Public: 328\n"
     ]
    }
   ],
   "source": [
    "###PRINT ALL CLASSIFICATION REPORTS, ACCURACY SCORES AND TARGET COUNTS###\n",
    "print(\"BEFORE ANALZYER:\")\n",
    "print(classificationBefore)\n",
    "print(\"The accuracy of prediction is: \", accuracyBefore)\n",
    "print(\"IST-WERT BEFORE ANALYZER:\")\n",
    "print(\"Person: \" + str(personIst))\n",
    "print(\"Group: \" + str(groupIst))\n",
    "print(\"Public: \" + str(publicIst))\n",
    "print(\"###############################################\")\n",
    "print(\"AFTER ANALYZER STEP 1:\")\n",
    "print(classificationAfter)\n",
    "print(\"The accuracy of prediction is: \", accuracyAfter)\n",
    "print(\"IST-WERT AFTER STEP 1:\")\n",
    "print(\"Person: \" + str(personIst2))\n",
    "print(\"Group: \" + str(groupIst2))\n",
    "print(\"Public: \" + str(publicIst2))\n",
    "print(\"AFTER ANALYZER STEP 2:\")\n",
    "print(classificationAfter2)\n",
    "print(\"The accuracy of prediction is: \", accuracyAfter2)\n",
    "print(\"IST-WERT AFTER STEP 2:\")\n",
    "print(\"Person: \" + str(personIst3))\n",
    "print(\"Group: \" + str(groupIst3))\n",
    "print(\"Public: \" + str(publicIst3))\n",
    "print(\"AFTER ANALYZER\")\n",
    "print(classificationAfter3)\n",
    "print(\"The accuracy of prediction is: \", accuracyAfter3)\n",
    "print(\"IST-WERT AFTER\")\n",
    "print(\"Person: \" + str(personIst4))\n",
    "print(\"Group: \" + str(groupIst4))\n",
    "print(\"Public: \" + str(publicIst4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
