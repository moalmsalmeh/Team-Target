{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00af3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2551d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# emb1 = model.encode(\"Test\")\n",
    "# emb2 = model.encode(\"Test\")\n",
    "\n",
    "# cos_sim = util.cos_sim(emb1, emb2)\n",
    "# print(\"Cosine-Similarity:\", cos_sim)\n",
    "# print(cos_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ NORMALIZED DATA FOR TARGET VERSION OF PIPELINE\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\normalizedResult.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243362f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TWO LISTS THAT CONTAIN EACH TEXT OF A TWEET AND ITS TARGET\n",
    "df=df.dropna()\n",
    "target=(df.iloc[0:8132 , 2 ].to_list())\n",
    "text=df.iloc[0:8132, 1 ].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d75b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME THAT CONTAINS TWEET TEXT AND TARGET\n",
    "df = pd.DataFrame ({'Text': text , 'Target': target })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5164e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Text, \n",
    "    df.Target, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2021,#-> same order of dataset random_state=2022, 2021\n",
    "    stratify=df.Target\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1be911ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TARGET SPLITTING OF 0.8 DATA###\n",
    "targetPerson=[]\n",
    "targetGroup=[]\n",
    "targetPublic=[]\n",
    "\n",
    "X_trainList = X_train.tolist()\n",
    "y_trainList = y_train.tolist()\n",
    "\n",
    "counter=0\n",
    "for t in X_trainList:\n",
    "    if y_trainList[counter] == 0:\n",
    "        targetPerson.append(t)\n",
    "    elif y_trainList[counter] == 1:\n",
    "        targetGroup.append(t)\n",
    "    else:\n",
    "        targetPublic.append(t)\n",
    "    counter+=1\n",
    "###TARGET SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99250b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "###UNIQUE WORD SPLITTING OF 0.8 DATA###\n",
    "personWords=[]\n",
    "groupWords=[]\n",
    "publicWords=[]\n",
    "\n",
    "for tweet in targetPerson:\n",
    "    personWords.append(tweet.split())\n",
    "personWords = sum(personWords, [])\n",
    "\n",
    "for tweet in targetGroup:\n",
    "    groupWords.append(tweet.split())\n",
    "groupWords = sum(groupWords, [])\n",
    "\n",
    "for tweet in targetPublic:\n",
    "    publicWords.append(tweet.split())\n",
    "publicWords = sum(publicWords, [])\n",
    "\n",
    "personUnique=[]\n",
    "groupUnique=[]\n",
    "publicUnique=[]\n",
    "\n",
    "for word in personWords:\n",
    "    if word not in personUnique:\n",
    "        personUnique.append(word)\n",
    "        \n",
    "for word in groupWords:\n",
    "    if word not in groupUnique:\n",
    "        groupUnique.append(word)\n",
    "        \n",
    "for word in publicWords:\n",
    "    if word not in publicUnique:\n",
    "        publicUnique.append(word)\n",
    "        \n",
    "onlyPerson=[]\n",
    "\n",
    "for word in personUnique:\n",
    "    if word not in groupUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyPerson.append(word)\n",
    "            \n",
    "onlyGroup=[]\n",
    "\n",
    "for word in groupUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyGroup.append(word)\n",
    "            \n",
    "onlyPublic=[]\n",
    "\n",
    "for word in publicUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in groupUnique:\n",
    "            onlyPublic.append(word)\n",
    "###UNIQUE WORD SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf00879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6464, 20470)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_classification \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tfidf\n",
    "#vecktorisieren\n",
    "\n",
    "vectorizer = TfidfVectorizer() \n",
    "Xtrain = vectorizer.fit_transform(X_train) \n",
    "Xtest = vectorizer.transform(X_test) \n",
    "print(Xtrain.shape)\n",
    "\n",
    "#vecktorisieren \n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('all-mpnet-base-v2')\n",
    "#model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "#model = SentenceTransformer('all-distilroberta-v1') multi-qa-distilbert-cos-v1\n",
    "#model = SentenceTransformer('multi-qa-distilbert-cos-v1') #multi-qa-MiniLM-L6-cos-v1\n",
    "#model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1') #paraphrase-multilingual-mpnet-base-v2\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') # best #paraphrase-albert-small-v2\n",
    "#model = SentenceTransformer('paraphrase-albert-small-v2') #paraphrase-multilingual-MiniLM-L12-v2\n",
    "#model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') #paraphrase-MiniLM-L3-v2\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L3-v2') #distiluse-base-multilingual-cased-v1\n",
    "#odel = SentenceTransformer('distiluse-base-multilingual-cased-v1') #distiluse-base-multilingual-cased-v2\n",
    "#odel = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "#print(X_train)\n",
    "x = X_train.tolist()\n",
    "Xtrain = model.encode(x) \n",
    "xt = X_test.tolist()\n",
    "Xtest = model.encode(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acecb6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Acc: 0.6586270871985158 for x: 0\n",
      "Current Acc: 0.645021645021645 for x: 10\n",
      "Current Acc: 0.6444032158317873 for x: 20\n",
      "Current Acc: 0.6388373531230674 for x: 30\n",
      "Current Acc: 0.6283240568954854 for x: 40\n",
      "Current Acc: 0.6277056277056277 for x: 50\n",
      "Current Acc: 0.6178107606679035 for x: 60\n",
      "Current Acc: 0.616573902288188 for x: 70\n",
      "Current Acc: 0.6110080395794681 for x: 80\n",
      "Current Acc: 0.6091527520098948 for x: 90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7260\\194824524.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;31m#2. fit with X_train and y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m#3. get the predictions for X_test and store it in y_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1514\u001b[0m             )\n\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1516\u001b[1;33m             self._Booster = train(\n\u001b[0m\u001b[0;32m   1517\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1518\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#create classifier and train model\n",
    "bestAcc=0\n",
    "bestX=0\n",
    "\n",
    "for x in range(21):\n",
    "    y = x*10\n",
    "    clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021, #booster='gblinear', 2021\n",
    "\n",
    "    #                           n_estimators = 1000, #1000: 0.6357452071737786\n",
    "                            \n",
    "                            \n",
    "    #                           gamma=0.9, #0.9 best with 0.6536796536796536\n",
    "                            \n",
    "                            \n",
    "    #                           min_child_weight=19, #19 best with 0.6524427952999382 \n",
    "                            \n",
    "                            \n",
    "    #                           learning_rate=0.2, #0.2 best with 0.6462585034013606\n",
    "                            \n",
    "                               #Best Acc: 0.6598639455782312 for x: 6\n",
    "                               reg_alpha=y, #17 best with 0.6524427952999382\n",
    "                            \n",
    "                            \n",
    "    #                           max_depth=11, #11 best with 0.6499690785405071\n",
    "\n",
    "\n",
    "                               colsample_bytree=0.7, colsample_bylevel=0.6, colsample_bynode=1 #0.7, 0.6, 1.0 best with 0.6586270871985158\n",
    "                           )\n",
    "\n",
    "    #2. fit with X_train and y_train\n",
    "    clf.fit(Xtrain, y_train)\n",
    "\n",
    "    #3. get the predictions for X_test and store it in y_pred\n",
    "    y_pred = clf.predict(Xtest)\n",
    "\n",
    "#     #4. classfication report BEFORE ANALYZER\n",
    "#     classificationBefore = classification_report(y_test, y_pred)\n",
    "\n",
    "    #5. find accuracy scores\n",
    "    accuracyBefore = accuracy_score(y_test, y_pred)\n",
    "    print(\"Current Acc: \" + str(accuracyBefore) + \" for x: \" + str(y))\n",
    "    if accuracyBefore > bestAcc:\n",
    "        bestAcc = accuracyBefore\n",
    "        bestX = y\n",
    "print(\"Best Acc: \" + str(bestAcc) + \" for x: \" + str(bestX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66fd0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. count prediction targets\n",
    "personIst=0\n",
    "groupIst=0\n",
    "publicIst=0\n",
    "for p in y_pred:\n",
    "    if p == 0:\n",
    "        personIst+=1\n",
    "    if p == 1:\n",
    "        groupIst+=1\n",
    "    if p == 2:\n",
    "        publicIst+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2794cef",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7260\\3258093374.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# y_pred2=y_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# countList=[]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# personCount=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# groupCount=0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "print(y_pred[0:100])\n",
    "# y_pred2=y_pred\n",
    "# countList=[]\n",
    "# personCount=0\n",
    "# groupCount=0\n",
    "# publicCount=0\n",
    "# counter=0\n",
    "# single_pronouns=[\"du\", \"dich\", \"deinen\"]\n",
    "# single_adress=[\"herr\", \"herrn\", \"frau\", \"junge\", \"nazi\"]\n",
    "# single_politicians=[\"merkel\", \"söder\", \"spahn\", \"soeder\", \"laschet\", \"baerbock\", \"bundeskanzlerin\", \"bundeskanzler\", \"kanzler\", \"kanzlerin\"]\n",
    "# group_pronouns=[\"euch\", \"eure\", \"euer\", \"deren\", \"ihr\"]\n",
    "# group_politics=[\"spd\", \"cdu\", \"csu\", \"afd\", \"grünen\", \"grüne\", \"union\", \"linken\", \"nazis\", \"rechten\", \"partei\", \"land\", \"terroristen\", \"demokratie\"]\n",
    "# group_countries=[\"deutschland\", \"deutsche\", \"bürger\"]\n",
    "# public_pronouns=[\"alle\", \"wir\", \"menschen\", \"volk\", \"bevölkerung\", \"welt\", \"bewohner\"]\n",
    "# testList=public_pronouns\n",
    "# Xtest = Xtest.array2string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "910902e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #7. run analyzer for target correction\n",
    "# #7.1 filter out person tweets out of group tweets and correct prediction\n",
    "# print(type(Xtest))\n",
    "# print(type(Xtest[0]))\n",
    "# for s in y_pred2:\n",
    "#     if y_pred2[counter] == 1:\n",
    "#         doc=Xtest[counter].split()\n",
    "#         for token in doc:\n",
    "#             if token in onlyPerson:\n",
    "#                 personCount+=1\n",
    "#             if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "#                 personCount+=1\n",
    "#         #7       (0.6239950525664811)\n",
    "#         #6       (0.6246134817563389)\n",
    "#         #5       (0.6246134817563389) <-- BEST\n",
    "#         #4       (0.6246134817563389)\n",
    "#         #3       (0.6239950525664811)\n",
    "#         if personCount >= 5:\n",
    "#             y_pred2[counter]=0\n",
    "#         personCount=0\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "\n",
    "# #7.1.1. classfication report AFTER ANALYZER (Step 1/3)\n",
    "# classificationAfter = classification_report(y_test, y_pred2)\n",
    "\n",
    "# #7.1.2. find accuracy scores\n",
    "# accuracyAfter = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "# #7.1.3. count prediction targets\n",
    "# personIst2=0\n",
    "# groupIst2=0\n",
    "# publicIst2=0\n",
    "# for p in y_pred2:\n",
    "#     if p == 0:\n",
    "#         personIst2+=1\n",
    "#     if p == 1:\n",
    "#         groupIst2+=1\n",
    "#     if p == 2:\n",
    "#         publicIst2+=1\n",
    "\n",
    "# #7.2. filter out group tweets out of person tweets and correct prediction\n",
    "# for s in y_pred2:\n",
    "#     if y_pred2[counter] == 0:\n",
    "#         doc=Xtest[counter].split()\n",
    "#         for token in doc:\n",
    "#             if token in onlyGroup:\n",
    "#                 groupCount+=1\n",
    "#             if token in group_pronouns or token in group_politics or token in group_countries:\n",
    "#                 groupCount+=1\n",
    "#         #6       (0.6246134817563389)\n",
    "#         #5       (0.6252319109461967) <-- BEST\n",
    "#         #4       (0.6252319109461967)\n",
    "#         #3       (0.6246134817563389)\n",
    "#         #2       (0.6171923314780458)\n",
    "#         if groupCount >= 5:\n",
    "#             y_pred2[counter]=1\n",
    "#         groupCount=0\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "\n",
    "# #7.2.1. classfication report AFTER ANALYZER (Step 2/3)\n",
    "# classificationAfter2 = classification_report(y_test, y_pred2)\n",
    "\n",
    "# #7.2.2. find accuracy scores\n",
    "# accuracyAfter2 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "# #7.2.3. count prediction targets\n",
    "# personIst3=0\n",
    "# groupIst3=0\n",
    "# publicIst3=0\n",
    "# for p in y_pred2:\n",
    "#     if p == 0:\n",
    "#         personIst3+=1\n",
    "#     if p == 1:\n",
    "#         groupIst3+=1\n",
    "#     if p == 2:\n",
    "#         publicIst3+=1\n",
    "\n",
    "# #7.3. filter out public tweets out of person and group tweets and correct prediction\n",
    "# for s in y_pred2:\n",
    "#     if y_pred2[counter] == 0 or y_pred2[counter] == 1:\n",
    "#         doc=Xtest[counter].split()\n",
    "#         for token in doc:\n",
    "#             if token in onlyPublic:\n",
    "#                 publicCount+=1\n",
    "#             if token in public_pronouns:\n",
    "#                 publicCount+=1\n",
    "#         #6       (0.6252319109461967)\n",
    "#         #5       (0.6258503401360545) <-- BEST\n",
    "#         #4       (0.6258503401360545)\n",
    "#         #3       (0.6190476190476191)\n",
    "#         if publicCount >= 5:\n",
    "#             y_pred2[counter]=2\n",
    "#         publicCount=0\n",
    "#     counter+=1\n",
    "# counter=0\n",
    "\n",
    "# #7.3.1. classfication report AFTER ANALYZER (Step 3/3)\n",
    "# classificationAfter3 = classification_report(y_test, y_pred2)\n",
    "\n",
    "# #7.3.2. find accuracy scores\n",
    "# accuracyAfter3 = accuracy_score(y_test, y_pred2)\n",
    "\n",
    "# #7.3.3. count prediction targets\n",
    "# personIst4=0\n",
    "# groupIst4=0\n",
    "# publicIst4=0\n",
    "# for p in y_pred2:\n",
    "#     if p == 0:\n",
    "#         personIst4+=1\n",
    "#     if p == 1:\n",
    "#         groupIst4+=1\n",
    "#     if p == 2:\n",
    "#         publicIst4+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e6e790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE ANALZYER:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.69       571\n",
      "           1       0.65      0.75      0.70       642\n",
      "           2       0.62      0.45      0.52       404\n",
      "\n",
      "    accuracy                           0.66      1617\n",
      "   macro avg       0.65      0.63      0.64      1617\n",
      "weighted avg       0.66      0.66      0.65      1617\n",
      "\n",
      "The accuracy of prediction is:  0.6586270871985158\n",
      "IST-WERT BEFORE ANALYZER:\n",
      "Person: 582\n",
      "Group: 742\n",
      "Public: 293\n",
      "###############################################\n"
     ]
    }
   ],
   "source": [
    "###PRINT ALL CLASSIFICATION REPORTS, ACCURACY SCORES AND TARGET COUNTS###\n",
    "print(\"BEFORE ANALZYER:\")\n",
    "print(classificationBefore)\n",
    "print(\"The accuracy of prediction is: \", accuracyBefore)\n",
    "print(\"IST-WERT BEFORE ANALYZER:\")\n",
    "print(\"Person: \" + str(personIst))\n",
    "print(\"Group: \" + str(groupIst))\n",
    "print(\"Public: \" + str(publicIst))\n",
    "print(\"###############################################\")\n",
    "# print(\"AFTER ANALYZER STEP 1:\")\n",
    "# print(classificationAfter)\n",
    "# print(\"The accuracy of prediction is: \", accuracyAfter)\n",
    "# print(\"IST-WERT AFTER STEP 1:\")\n",
    "# print(\"Person: \" + str(personIst2))\n",
    "# print(\"Group: \" + str(groupIst2))\n",
    "# print(\"Public: \" + str(publicIst2))\n",
    "# print(\"AFTER ANALYZER STEP 2:\")\n",
    "# print(classificationAfter2)\n",
    "# print(\"The accuracy of prediction is: \", accuracyAfter2)\n",
    "# print(\"IST-WERT AFTER STEP 2:\")\n",
    "# print(\"Person: \" + str(personIst3))\n",
    "# print(\"Group: \" + str(groupIst3))\n",
    "# print(\"Public: \" + str(publicIst3))\n",
    "# print(\"AFTER ANALYZER\")\n",
    "# print(classificationAfter3)\n",
    "# print(\"The accuracy of prediction is: \", accuracyAfter3)\n",
    "# print(\"IST-WERT AFTER\")\n",
    "# print(\"Person: \" + str(personIst4))\n",
    "# print(\"Group: \" + str(groupIst4))\n",
    "# print(\"Public: \" + str(publicIst4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
