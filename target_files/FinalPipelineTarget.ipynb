{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00af3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff2ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#READ NORMALIZED DATA FOR TARGET VERSION OF PIPELINE\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\normalizedResult.tsv\", sep=\"\\t\")\n",
    "testdf = pd.read_csv(\"C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\TestDaten\\\\Testdaten.tsv\", usecols=['c_text'], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7bc98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "texts=testdf['c_text']\n",
    "tweet_words = []\n",
    "n_text = \"\"\n",
    "for tweet in texts:\n",
    "    doc = nlp(tweet)\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            n_text+=token.text.lower()+\" \"\n",
    "    \n",
    "    tweet_words.append(n_text)\n",
    "    n_text=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9631b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0\n",
    "for t in tweet_words:\n",
    "    tweet_words[counter] = t[:-1]\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2211f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf['c_text']=tweet_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "243362f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE TWO LISTS THAT CONTAIN EACH TEXT OF A TWEET AND ITS TARGET\n",
    "df=df.dropna()\n",
    "target=(df.iloc[0:8132 , 2 ].to_list())\n",
    "text=df.iloc[0:8132, 1 ].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50c9ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf=testdf.dropna()\n",
    "testtext=testdf['c_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d75b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME THAT CONTAINS TWEET TEXT AND TARGET\n",
    "df=pd.DataFrame ({'Text': text , 'Target': target })\n",
    "testdf=pd.DataFrame ({'Text': testtext })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5164e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.Text\n",
    "y_train = df.Target\n",
    "X_test = testdf.Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1be911ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TARGET SPLITTING OF 0.8 DATA###\n",
    "targetPerson=[]\n",
    "targetGroup=[]\n",
    "targetPublic=[]\n",
    "\n",
    "X_trainList = X_train.tolist()\n",
    "y_trainList = y_train.tolist()\n",
    "\n",
    "counter=0\n",
    "for t in X_trainList:\n",
    "    if y_trainList[counter] == 0:\n",
    "        targetPerson.append(t)\n",
    "    elif y_trainList[counter] == 1:\n",
    "        targetGroup.append(t)\n",
    "    else:\n",
    "        targetPublic.append(t)\n",
    "    counter+=1\n",
    "###TARGET SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99250b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "###UNIQUE WORD SPLITTING OF 0.8 DATA###\n",
    "personWords=[]\n",
    "groupWords=[]\n",
    "publicWords=[]\n",
    "\n",
    "for tweet in targetPerson:\n",
    "    personWords.append(tweet.split())\n",
    "personWords = sum(personWords, [])\n",
    "\n",
    "for tweet in targetGroup:\n",
    "    groupWords.append(tweet.split())\n",
    "groupWords = sum(groupWords, [])\n",
    "\n",
    "for tweet in targetPublic:\n",
    "    publicWords.append(tweet.split())\n",
    "publicWords = sum(publicWords, [])\n",
    "\n",
    "personUnique=[]\n",
    "groupUnique=[]\n",
    "publicUnique=[]\n",
    "\n",
    "for word in personWords:\n",
    "    if word not in personUnique:\n",
    "        personUnique.append(word)\n",
    "        \n",
    "for word in groupWords:\n",
    "    if word not in groupUnique:\n",
    "        groupUnique.append(word)\n",
    "        \n",
    "for word in publicWords:\n",
    "    if word not in publicUnique:\n",
    "        publicUnique.append(word)\n",
    "        \n",
    "onlyPerson=[]\n",
    "\n",
    "for word in personUnique:\n",
    "    if word not in groupUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyPerson.append(word)\n",
    "            \n",
    "onlyGroup=[]\n",
    "\n",
    "for word in groupUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in publicUnique:\n",
    "            onlyGroup.append(word)\n",
    "            \n",
    "onlyPublic=[]\n",
    "\n",
    "for word in publicUnique:\n",
    "    if word not in personUnique:\n",
    "        if word not in groupUnique:\n",
    "            onlyPublic.append(word)\n",
    "###UNIQUE WORD SPLITTING END###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bf00879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034\n",
      "2034\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_classification \n",
    "import xgboost as xgb\n",
    "\n",
    "print(len(X_test))\n",
    "#tfidf\n",
    "#vektorisieren\n",
    "vectorizer = TfidfVectorizer() \n",
    "Xtrain = vectorizer.fit_transform(X_train) \n",
    "Xtest = vectorizer.transform(X_test) \n",
    "#vektorisieren\n",
    "print(len(X_test))\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "x = X_train.tolist()\n",
    "Xtrain = model.encode(x) \n",
    "xt = X_test.tolist()\n",
    "Xtest = model.encode(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d558f8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2034\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acecb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create classifier and train model\n",
    "\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021,\n",
    "                           n_estimators=100,\n",
    "                           gamma=0,\n",
    "                           min_child_weight=1,\n",
    "                           learning_rate=0.3,\n",
    "                           reg_alpha=6,\n",
    "                           max_depth=6,\n",
    "                           colsample_bytree=0.7, colsample_bylevel=0.6, colsample_bynode=1\n",
    "                       )\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "clf.fit(Xtrain, y_train)\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "y_pred = clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2794cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2=y_pred\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "counter=0\n",
    "single_pronouns=[\"du\", \"dich\", \"dein\", \"er\", \"sein\", \"mein\", \"meine\", \"mir\"]\n",
    "single_adress=[\"herr\", \"herrn\", \"frau\", \"junge\", \"nazi\"]\n",
    "single_politicians=[\"merkel\", \"söder\", \"spahn\", \"soeder\", \"laschet\", \"baerbock\", \"bundeskanzlerin\", \"bundeskanzler\", \"kanzler\", \"kanzlerin\", \"putin\", \"trump\", \"erdogan\"]\n",
    "group_pronouns=[\"euch\", \"eure\", \"euer\", \"deren\", \"ihr\", \"unsere\", \"uns\"]\n",
    "group_politics=[\"spd\", \"cdu\", \"csu\", \"afd\", \"grünen\", \"grüne\", \"union\", \"linken\", \"nazis\", \"rechten\", \"partei\", \"land\", \"terroristen\", \"demokratie\"]\n",
    "group_religion=[\"muslime\", \"islamische\", \"islam\", \"islamistische\", \"christen\", \"juden\"]\n",
    "group_countries=[\"deutschland\", \"deutsche\", \"bürger\", \"araber\", \"ausländer\", \"schwarzen\"]\n",
    "public_pronouns=[\"alle\", \"wir\", \"menschen\", \"volk\", \"bevölkerung\", \"welt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910902e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. run analyzer for target correction\n",
    "#7.1. filter out person tweets out of group tweets and correct prediction\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 1:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1\n",
    "        if personCount >= 5:\n",
    "            y_pred2[counter]=0\n",
    "        personCount=0\n",
    "    counter+=1\n",
    "counter=0\n",
    "\n",
    "#7.2. filter out group tweets out of person tweets and correct prediction\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 0:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=1\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries or token in group_religion:\n",
    "                groupCount+=1\n",
    "        if groupCount >= 5:\n",
    "            y_pred2[counter]=1\n",
    "        groupCount=0\n",
    "    counter+=1\n",
    "counter=0\n",
    "\n",
    "#7.3. filter out public tweets out of person and group tweets and correct prediction\n",
    "personCount=0\n",
    "groupCount=0\n",
    "publicCount=0\n",
    "for s in y_pred2:\n",
    "    if y_pred2[counter] == 0 or y_pred2[counter] == 1:\n",
    "        doc=X_test[counter].split()\n",
    "        for token in doc:\n",
    "            if token in onlyPublic:\n",
    "                publicCount+=1\n",
    "            if token in public_pronouns:\n",
    "                publicCount+=1\n",
    "            if token in onlyGroup:\n",
    "                groupCount+=1\n",
    "            if token in group_pronouns or token in group_politics or token in group_countries or token in group_religion:\n",
    "                groupCount+=1\n",
    "            if token in onlyPerson:\n",
    "                personCount+=1\n",
    "            if token in single_pronouns or token in single_adress or token in single_politicians:\n",
    "                personCount+=1\n",
    "        if publicCount > groupCount - 3 or publicCount > personCount - 3:\n",
    "            y_pred2[counter]=2\n",
    "        publicCount=0\n",
    "    counter+=1\n",
    "counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb409c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfFinal=pd.DataFrame({'Klasse': y_pred2})\n",
    "testdfFinal.to_csv('C:\\\\Users\\\\brush\\\\Projekt_NLP\\\\Result\\\\Hybrides Team Target_1b.csv', sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
