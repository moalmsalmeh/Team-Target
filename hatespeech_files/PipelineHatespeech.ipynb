{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Entwicklungsdaten.tsv\", sep=\"\\t\")\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Trainingsdaten-GermEvalDatei2018.tsv\", sep=\"\\t\", names=[\"c_text\",\"hate\",\"type\",\"score\"])\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Trainingsdaten-GermEvalDatei2019.tsv\", sep=\"\\t\", names=[\"c_text\",\"hate\",\"type\",\"score\"])\n",
    "df3 = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Testdaten.tsv\", sep=\"\\t\")\n",
    "#print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d17aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_frames = [df, df1, df2]\n",
    "result = pd.concat(joined_frames)\n",
    "text= result.iloc[0:, 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempArray = []\n",
    "for row in range(len(df1)):\n",
    "    #print(df1.iloc[row, 2])\n",
    "    selected_row =df1.iloc[row, 2]\n",
    "    if selected_row == \"INSULT\" or selected_row == \"ABUSE\":\n",
    "        tempArray.append(1)\n",
    "    else:\n",
    "        tempArray.append(0)\n",
    "#print(tempArray)\n",
    "\n",
    "df1['label'] = pd.Series(tempArray)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempArray = []\n",
    "for row in range(len(df2)):\n",
    "    #print(df1.iloc[row, 2])\n",
    "    selected_row =df2.iloc[row, 2]\n",
    "    if selected_row == \"INSULT\" or selected_row == \"ABUSE\":\n",
    "        tempArray.append(1)\n",
    "    else:\n",
    "        tempArray.append(0)\n",
    "#print(tempArray)\n",
    "\n",
    "df2['label'] = pd.Series(tempArray)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = (df.iloc[0: , 8 ].to_list() + df1.iloc[0: , 4].to_list() + df2.iloc[0: , 4].to_list())\n",
    "#print(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ab60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "#normalize dataset\n",
    "array =[]\n",
    "for t in text[:]:\n",
    "    doc = nlp(t)\n",
    "    \n",
    "    #with stop words\n",
    "    #temp = [token.lemma_.lower() for token in doc if token.is_alpha] #36\n",
    "    \n",
    "    #without stop words and only alpha\n",
    "    #temp = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    #without stop words and with smileys /no alpha chars\n",
    "    temp = [token.lemma_.lower() for token in doc if not token.is_stop] # 0.45 f1\n",
    "    \n",
    "    #with smileys and stopwords\n",
    "    #temp = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    #print(temp)\n",
    "\n",
    "    array.append((\" \".join(temp)))\n",
    "\n",
    "#print (array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e622e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further normalization of the data gave no improvement\n",
    "\n",
    "# word1 = \"http\"\n",
    "# word2 = \"--\"\n",
    "# tweetWords=[] \n",
    "# words=[]\n",
    "# normArray =[]\n",
    "# delete = False\n",
    "\n",
    "# for tweet in array:     \n",
    "#     tweetWords.append(tweet.split())   \n",
    "# c1=0\n",
    "# for t in tweetWords:     \n",
    "#     c2=0    \n",
    "#     for w in t:\n",
    "# #         if \"@\" in w:\n",
    "# #             delete=True\n",
    "#         if \"http\" in w:\n",
    "#             delete=True\n",
    "#         if delete == False:\n",
    "#             words.append(w.replace(\"--\",\"\").replace(\"+\", \"\"))#.replace(\"#\",\"\").replace(\"[\",\"\").replace(\"]\",\"\"))\n",
    "# #             words.append(w.replace(\"--\",\"\").replace(\"#\",\"\").\n",
    "# #                          replace(\"+\", \"\").replace(\"_\",\"\").replace(\".\",\"\").\n",
    "# #                          replace(\"/\",\"\").replace(\"[\",\"\").replace(\"]\",\"\")) \n",
    "            \n",
    "#         delete=False\n",
    "#         c2+=1   \n",
    "    \n",
    "#     normArray.append(\" \".join(words))\n",
    "#     words.clear()\n",
    "#     c1+=1     \n",
    "# print(normArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame ({'Text': array , 'Hatespeech': hs })\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train und test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Text, \n",
    "    df.Hatespeech, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2021,\n",
    "    stratify=df.Hatespeech\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedc85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As this dataset is highly imbalance we have to balance this by over sampling\n",
    "\n",
    "trainingdata = {\n",
    "    \"string\": X_train,\n",
    "    \"label\": y_train\n",
    "}\n",
    "newDf = pd.DataFrame(trainingdata)\n",
    "\n",
    "print(newDf)\n",
    "\n",
    "cnt_non_fraud = newDf[newDf['label'] == 0]['string'].count()\n",
    "df_class_fraud = newDf[newDf['label'] == 1]\n",
    "df_class_nonfraud = newDf[newDf['label'] == 0]\n",
    "df_class_fraud_oversample = df_class_fraud.sample(cnt_non_fraud, replace=True)\n",
    "df_oversampled = pd.concat([df_class_nonfraud, df_class_fraud_oversample], axis=0)\n",
    "\n",
    "print('Random over-sampling:')\n",
    "print(df_oversampled['label'].value_counts())\n",
    "print(df_oversampled)\n",
    "\n",
    "X_train = df_oversampled['string']\n",
    "y_train = df_oversampled['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de6169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98141676",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64018f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c081b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #with sbert we could not achieve better results than with tfidf (see down below)\n",
    "\n",
    "# from sklearn.metrics import accuracy_score \n",
    "# from sklearn.metrics import classification_report \n",
    "# from sklearn.neighbors import KNeighborsClassifier \n",
    "# from sklearn.pipeline import Pipeline \n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier \n",
    "# from sklearn.datasets import make_classification \n",
    "# import xgboost as xgb\n",
    "# from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# # tested pretrained models of sbert\n",
    "\n",
    "# #model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "# #model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# #model = SentenceTransformer('all-mpnet-base-v2')\n",
    "# #model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "# #model = SentenceTransformer('all-distilroberta-v1') multi-qa-distilbert-cos-v1\n",
    "# #model = SentenceTransformer('multi-qa-distilbert-cos-v1') #multi-qa-MiniLM-L6-cos-v1\n",
    "# #model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1') #paraphrase-multilingual-mpnet-base-v2\n",
    "# model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') # best #paraphrase-albert-small-v2\n",
    "# #model = SentenceTransformer('paraphrase-albert-small-v2') #paraphrase-multilingual-MiniLM-L12-v2\n",
    "# #model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') #paraphrase-MiniLM-L3-v2\n",
    "# #model = SentenceTransformer('paraphrase-MiniLM-L3-v2') #distiluse-base-multilingual-cased-v1\n",
    "# #model = SentenceTransformer('distiluse-base-multilingual-cased-v1') #distiluse-base-multilingual-cased-v2\n",
    "# #model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "# #print(X_train)\n",
    "\n",
    "# #vectorize data with sbert\n",
    "# x = X_train.tolist()\n",
    "# Xtrain = model.encode(x) \n",
    "# xt = X_test.tolist()\n",
    "# Xtest = model.encode(xt)\n",
    "\n",
    "#  #('KNN', KNeighborsClassifier())\n",
    "#  #('SVM', SVC())   \n",
    "#  #('rfc', RandomForestClassifier()) \n",
    "\n",
    "# #create classifier and train model \n",
    "# clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021, #tree_method=\"gpu_hist\", enable_categorical=True, #booster='gblinear', 2021\n",
    "#           #                n_estimators = 1000,\n",
    "#                            gamma=4.2, \n",
    "#                            min_child_weight=1,                         \n",
    "#                            learning_rate =0.5, \n",
    "#                            max_depth=6, \n",
    "#           #                colsample_bytree=1, \n",
    "#           #                colsample_bylevel=1, \n",
    "#           #                colsample_bynode=1, \n",
    "\n",
    "#     )\n",
    "\n",
    "# clf.fit(Xtrain, y_train)\n",
    "\n",
    "# y_pred = clf.predict(Xtest)\n",
    "\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred) \n",
    "# print(\"The accuracy of prediction is: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfee0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_classification \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "#vectorize data with tfidf\n",
    "vectorizer = TfidfVectorizer() \n",
    "Xtrain = vectorizer.fit_transform(X_train) \n",
    "Xtest = vectorizer.transform(X_test) \n",
    "print(Xtrain.shape)\n",
    "\n",
    "#create classifier and train model with best found params\n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021,# tree_method=\"gpu_hist\", enable_categorical=True,  #booster='gblinear', 2021\n",
    "                        n_estimators = 1000,\n",
    "                        gamma=0.63, \n",
    "                        min_child_weight=0.1,\n",
    "                        reg_alpha=0,\n",
    "                        max_depth=14, #14 27\n",
    "                        colsample_bytree=1, colsample_bylevel=0.5, colsample_bynode=1\n",
    "                         )\n",
    "                        \n",
    "#clf = KNeighborsClassifier()\n",
    "#clf =  SVC()\n",
    "#clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "clf.fit(Xtrain, y_train)\n",
    "\n",
    "#predictions for X_test and store it in y_pred \n",
    "y_pred = clf.predict(Xtest)\n",
    "\n",
    "#print the classfication report \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#print accuracy scores\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(\"The accuracy of prediction is: \", accuracy)\n",
    "\n",
    "#count prediction\n",
    "hs=1\n",
    "nhs=0\n",
    "\n",
    "for p in y_pred:\n",
    "    if p == 1:\n",
    "        hs+=1\n",
    "    if p == 0:\n",
    "        nhs+=1\n",
    "print(\"hs: \", hs)\n",
    "print(\"nhs: \", nhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb9305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d763f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for single input\n",
    "\n",
    "# while True:\n",
    "#     choice = input(\"Enter Choice: \")\n",
    "#     #print(choice)\n",
    "    \n",
    "#     array =[]\n",
    "#     #for t in choice[:]:\n",
    "#     doc = nlp(choice)\n",
    "\n",
    "#     #without stop words and with smileys /no alpha chars\n",
    "#     temp = [token.lemma_.lower() for token in doc if not token.is_stop] # 0.45 f1\n",
    "\n",
    "#     array.append((\" \".join(temp)))\n",
    "    \n",
    "#     dataframe = pd.DataFrame ({'Text': array })\n",
    "\n",
    "#    # print (array)\n",
    "    \n",
    "#     #print(dataframe)\n",
    "#     Xtest = vectorizer.transform(array) \n",
    "#     #print(Xtest)\n",
    "#     y_pred = clf.predict(Xtest)\n",
    "#     print(y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7664606",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict Testdaten.tsv\n",
    "\n",
    "# text= df3.iloc[0:, 1 ]\n",
    "# print(text)\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# array =[]\n",
    "# for t in text[:]:\n",
    "#     doc = nlp(t)\n",
    "    \n",
    "#     #without stop words and with smileys /no alpha chars\n",
    "#     temp = [token.lemma_.lower() for token in doc if not token.is_stop] # 0.45 f1\n",
    "    \n",
    "#     array.append((\" \".join(temp)))\n",
    "\n",
    "\n",
    "# Xtestdata = vectorizer.transform(array) \n",
    "# print(Xtestdata.shape)\n",
    "\n",
    "# y_predtest = clf.predict(Xtestdata)\n",
    "# c =0\n",
    "# for i in y_predtest[0:]:\n",
    "#     print(c,\"->\", i)\n",
    "#     c+=1\n",
    "    \n",
    "# testdfFinal=pd.DataFrame({'Klasse': y_predtest})\n",
    "# print(testdfFinal)\n",
    "# testdfFinal.to_csv('C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\results\\\\results\\\\Hybrides Team Target_1a.csv', sep =\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae01542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a pipeline with gridsearch to find best params\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     #('KNN', KNeighborsClassifier())\n",
    "     #('SVM', SVC())   \n",
    "     #('rfc', RandomForestClassifier()) \n",
    "     ('xgb', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021))  #42 tree_method='gpu_hist'\n",
    "])\n",
    "\n",
    "# set parameter combination and their range\n",
    "\n",
    "params = {\n",
    "    #'vectorizer_tfidf__ngram_range': [(1, 1)], #, (1, 2), (2, 2)\n",
    "    \n",
    "#      'xgb__gamma': [0.3], # 0.5, 1.4, 1.7   \n",
    "#      'xgb__max_depth': [12,1,6,13],\n",
    "#      'xgb__min_child_weight': [0.1], \n",
    "#      'xgb__max_delta_step': [0] ,\n",
    "#      'xgb__reg_alpha': [0.5], \n",
    "#      'xgb__reg_lambda':[0.1],\n",
    "#      'xgb__colsample_bytree':[0.1],  #i/10.0 for i in range(1,10)\n",
    "#     'xgb__subsample': [0.1],\n",
    "    \n",
    "    \n",
    "#    # 'xgb__n_estimators': [500], \n",
    "#          'xgb__gamma': [i/100.0 for i in range(200,300)]\n",
    "#        'xgb__min_child_weight': [0.46], #i/50.0 for i in range(1,50)\n",
    "#    # 'xgb__learning_rate' : [0.3],\n",
    "#         'xgb__max_depth': [27],\n",
    "#         'xgb__colsample_bytree':[1], \n",
    "#         'xgb__colsample_bylevel':[0.5], \n",
    "#         'xgb__colsample_bynode':[1],\n",
    "#     #'xgb__max_bin': [256, 512, 1024],\n",
    "#     #'xgb__max_cat_to_onehot': [2],\n",
    "#     #'xgb__max_delta_step':[0,1,2,3,4,5,6,7,8,9,10]\n",
    "    \n",
    "    #best combination\n",
    "    'xgb__n_estimators': [1000],\n",
    "    'xgb__gamma':[0],\n",
    "    'xgb__min_child_weight': [ 0.1],  \n",
    "    'xgb__max_depth': [14], \n",
    "    'xgb__colsample_bytree':[1], \n",
    "    'xgb__colsample_bylevel':[0.5], \n",
    "    'xgb__colsample_bynode':[1],\n",
    "    \n",
    "    \n",
    "\n",
    "#     #gpu hist\n",
    "#     'xgb__n_estimators': [189], #1000, 1100, 1200] \n",
    "#     'xgb__learning_rate' : [0.32], #i/100 for i in range(1,40)\n",
    "#     'xgb__gamma':[0.01], # 0-1 -> 0.01   i/100.0 for i in range(1,100)\n",
    "#     'xgb__min_child_weight': [0.1],\n",
    "#     'xgb__reg_alpha':[0.6],  #i/10 for i in range(1,10)\n",
    "#     'xgb__reg_lambda':[0.2], # 1-10 0.2\n",
    "#     'xgb__max_depth': [10], #14 i for i in range(0,14) -> 10\n",
    "#     'xgb__colsample_bytree':[0.9], #\n",
    "#     'xgb__colsample_bylevel':[0.3],   #0-1 xgb__colsample_bylevel': 0.3, 'xgb__colsample_bynode': 0.2,\n",
    "#     'xgb__colsample_bynode':[0.2],\n",
    "#    'xgb__max_delta_step': [4] , #0-10\n",
    "#     'xgb__subsample': [0.2], #0-10 -> 0.2\n",
    "    \n",
    "    \n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'vectorizer_tfidf__max_n': (1, 2),\n",
    "    # 'vectorizer_tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vectorizer_tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd57a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is needed for using tree_method='gpu_hist'\n",
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up GridSearch\n",
    "gs = GridSearchCV(clf, params, \n",
    "                  cv = 5, #cross-validation = 5\n",
    "                  verbose = 2,\n",
    "                  n_jobs = 1, \n",
    "                  scoring = 'f1',\n",
    "               \n",
    "                  #  scoring = [\"r2\", \"neg_root_mean_squared_error\"], #sklearn.metrics.SCORERS.keys()                                                  #scoring = ['accuracy', 'precision'],\n",
    "                 #  refit = \"r2\"\n",
    "                 \n",
    ")\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best score: %0.3f' % gs.best_score_)\n",
    "print ('Best parameters set:')\n",
    "best_parameters = gs.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a618e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_estimator_) # to get the complete details of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_estimator_.steps)\n",
    "best_clf = gs.best_estimator_\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "# print the classfication report\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"The accuracy of prediction is: \", accuracy)\n",
    "\n",
    "# save gs results\n",
    "import numpy as np\n",
    "\n",
    " df = pd.DataFrame(gs.cv_results_)\n",
    " df = df.sort_values(\"rank_test_score\")\n",
    " df.to_csv(\"xgb_results.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c6571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
