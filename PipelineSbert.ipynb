{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af3405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# #Sentences are encoded by calling model.encode()\n",
    "# emb1 = model.encode(\"Test\")\n",
    "# emb2 = model.encode(\"Test\")\n",
    "\n",
    "# cos_sim = util.cos_sim(emb1, emb2)\n",
    "# print(\"Cosine-Similarity:\", cos_sim)\n",
    "# print(cos_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2ac2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Entwicklungsdaten.tsv\", sep=\"\\t\")\n",
    "df1 = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Trainingsdaten-GermEvalDatei2018.tsv\", sep=\"\\t\", names=[\"c_text\",\"hate\",\"type\",\"score\"])\n",
    "df2 = pd.read_csv(\"C:\\\\Users\\\\User\\\\Desktop\\\\nlp\\\\Trainingsdaten-GermEvalDatei2019.tsv\", sep=\"\\t\", names=[\"c_text\",\"hate\",\"type\",\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d17aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_frames = [df, df1, df2] \n",
    "result = pd.concat(joined_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61970f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= result.iloc[0:, 1 ] #result\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f9211a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tempArray = []\n",
    "for row in range(len(df1)):\n",
    "    #print(df1.iloc[row, 2])\n",
    "    selected_row =df1.iloc[row, 2]\n",
    "    if selected_row == \"INSULT\" or selected_row == \"ABUSE\":\n",
    "        tempArray.append(1)\n",
    "    else:\n",
    "        tempArray.append(0)\n",
    "#print(tempArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6c4d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['label'] = pd.Series(tempArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ecd497",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempArray = []\n",
    "for row in range(len(df2)):\n",
    "    #print(df1.iloc[row, 2])\n",
    "    selected_row =df2.iloc[row, 2]\n",
    "    if selected_row == \"INSULT\" or selected_row == \"ABUSE\":\n",
    "        tempArray.append(1)\n",
    "    else:\n",
    "        tempArray.append(0)\n",
    "#print(tempArray)\n",
    "\n",
    "df2['label'] = pd.Series(tempArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1da72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3c49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs =  (df.iloc[0: , 8 ].to_list() + df1.iloc[0: , 4].to_list() + df2.iloc[0: , 4].to_list() ) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9891791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e541fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33ab60c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "array =[]\n",
    "for t in text[:]:\n",
    "    doc = nlp(t)\n",
    "    #print([token.text for token in doc])\n",
    "    \n",
    "    #normalize dataset\n",
    "    \n",
    "    #with stop words\n",
    "    #temp = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "    \n",
    "    #without stop words and only alpha\n",
    "    #temp = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
    "    \n",
    "    #without stop words and with smileys /no alpha chars\n",
    "    temp = [token.lemma_.lower() for token in doc if not token.is_stop ] #and not token.text == '@'\n",
    "    \n",
    "    #with smileys and stopwords\n",
    "    #temp = [token.lemma_.lower() for token in doc]\n",
    "    \n",
    "    #print(temp)\n",
    "\n",
    "    array.append((\" \".join(temp)))\n",
    "\n",
    "print (array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d75b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame ({'Text': array , 'Hatespeech': hs })\n",
    "print (df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07801a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.Text, \n",
    "    df.Hatespeech, \n",
    "    test_size=0.2, # 20% samples will go to test dataset\n",
    "    random_state=2021,#-> same order of dataset random_state=2022, 2021\n",
    "    stratify=df.Hatespeech\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de6169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of X_train: \", y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of X_train: \", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aad043",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((X_train.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98141676",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64018f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf00879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.datasets import make_classification \n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tfidf\n",
    "#vecktorisieren\n",
    "\n",
    "# vectorizer = TfidfVectorizer() \n",
    "# Xtrain = vectorizer.fit_transform(X_train) \n",
    "# Xtest = vectorizer.transform(X_test) \n",
    "# print(Xtrain.shape)\n",
    "\n",
    "#vecktorisieren \n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "#model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "#model = SentenceTransformer('all-mpnet-base-v2')\n",
    "#model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "#model = SentenceTransformer('all-distilroberta-v1') multi-qa-distilbert-cos-v1\n",
    "#model = SentenceTransformer('multi-qa-distilbert-cos-v1') #multi-qa-MiniLM-L6-cos-v1\n",
    "#model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1') #paraphrase-multilingual-mpnet-base-v2\n",
    "model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2') # best #paraphrase-albert-small-v2\n",
    "#model = SentenceTransformer('paraphrase-albert-small-v2') #paraphrase-multilingual-MiniLM-L12-v2\n",
    "#model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') #paraphrase-MiniLM-L3-v2\n",
    "#model = SentenceTransformer('paraphrase-MiniLM-L3-v2') #distiluse-base-multilingual-cased-v1\n",
    "#odel = SentenceTransformer('distiluse-base-multilingual-cased-v1') #distiluse-base-multilingual-cased-v2\n",
    "#odel = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "#print(X_train)\n",
    "x = X_train.tolist()\n",
    "Xtrain = model.encode(x) \n",
    "xt = X_test.tolist()\n",
    "Xtest = model.encode(xt)\n",
    "\n",
    "\n",
    "\n",
    "#create classifier and train model \n",
    "clf = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021, #booster='gblinear', 2021\n",
    "                        \n",
    "#                         n_estimators = 1000,\n",
    "#                         gamma=0.61, #2.7\n",
    "#                         min_child_weight=0.1,                         \n",
    "#                         learning_rate =0.3, #0,55 with bytree 1.... 0.3/0.38 best\n",
    "#                         reg_alpha=0,\n",
    "#                         max_depth=12,\n",
    " \n",
    "                        \n",
    "#                         colsample_bytree=1, colsample_bylevel=0.5, colsample_bynode=1\n",
    "                       )\n",
    "\n",
    "clf.fit(Xtrain, y_train)\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred \n",
    "y_pred = clf.predict(Xtest)\n",
    "\n",
    "#4. print the classfication report \n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#find accuracy scores\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(\"The accuracy of prediction is: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d830076",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59172e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = classification_report(y_test, y_pred)\n",
    "# dff = pd.DataFrame(result)\n",
    "# dff.to_csv(\"xgb_score.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60017c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = (classification_report(y_test, y_pred))\n",
    "#df5 = (pd.DataFrame\n",
    "\n",
    "# df = df.sort_values(\"rank_test_r2\")\n",
    "# df.to_csv(\"xgb_results.txt\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e15ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c6850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#1. create a pipeline object\n",
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     #('KNN', KNeighborsClassifier())\n",
    "     #('SVM', SVC())   \n",
    "     #('rfc', RandomForestClassifier()) \n",
    "     ('xgb', xgb.XGBClassifier(objective=\"binary:logistic\", random_state=2021))  #42 tree_method='gpu_hist'\n",
    "])\n",
    "\n",
    "#2. fit with X_train and y_train\n",
    "#clf.fit(X_train, y_train)\n",
    "\n",
    "#rf\n",
    "#  'rf__max_depth': [1000],\n",
    "#  'rf__min_samples_split': [100],\n",
    "#  'rf__max_leaf_nodes': [None]\n",
    "\n",
    "# Setting up gxboost params\n",
    "#'xgb__min_samples_split': [100],\n",
    "    #'xgb__max_leaf_nodes': [None],\n",
    "#'vectorizer_tfidf__stop_words': ['english'],\n",
    " #'vectorizer_tfidf__max_features':[2000],\n",
    " #'vectorizer_tfidf__ngram_range': [(1, 2)], \n",
    "\n",
    "#     \"n_estimators\" : [100, 200, 500],\n",
    "#     \"max_depth\" : [3, 6, 9],\n",
    "#     \"gamma\" : [0.01, 0.1],\n",
    "#     \"learning_rate\" : [0.001, 0.01, 0.1, 1]\n",
    "#   'xgb__colsample_bytree' : [0.3, 0.5, 0.8],\n",
    "#   'vectorizer_tfidf__max_features':[100, 2000],\n",
    "\n",
    "# l1 und l2 lambda\n",
    "#max def \n",
    "#ngrams tfid 1 und 2\n",
    "#  scale_pos_weight\n",
    "\n",
    "params = {\n",
    "    #'vectorizer_tfidf__ngram_range': [(1, 1)], #, (1, 2), (2, 2)\n",
    "    \n",
    "#      'xgb__gamma': [0.3], # 0.5, 1.4, 1.7    #, 1.4 , 1.7 also good - maybe test again ,i/10.0 for i in range(0,5) , # 1.9, 1.8,  2, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7\n",
    "#      'xgb__max_depth': [12,1,6,13], #3, 4 , 6 # used 1,5,6,10,12, 14\n",
    "#      'xgb__min_child_weight': [0.1], # 0.1 / 2\n",
    "#      'xgb__max_delta_step': [0] ,\n",
    "#      'xgb__reg_alpha': [0.5], #0, 0.1, 0.2, 0.3, 0.4, wanna test #1e-5, 1e-2, 0.1, 1, 100 | 1.9 - 2.1 - 2.0 (1, 1.2, 1.3, 1.5, 1.7, 1.9, 2.0, 2.1)\n",
    "#      'xgb__reg_lambda':[0.1],\n",
    "#      'xgb__colsample_bytree':[0.1],  #0.5,0.6,0.7, 0.8, 0.9, 1 i/10.0 for i in range(1,10)\n",
    "#     'xgb__subsample': [0.1],\n",
    "    \n",
    "    \n",
    "    'xgb__n_estimators': [1000], \n",
    "     'xgb__gamma': [0.61],\n",
    "    'xgb__min_child_weight': [0.1],\n",
    "    'xgb__reg_alpha': [0],\n",
    "     'xgb__max_depth': [1,6,11,12,13,14,15,16,17],\n",
    "     'xgb__colsample_bytree':[1],\n",
    "     'xgb__colsample_bylevel':[0.5], \n",
    "    'xgb__colsample_bynode':[1],\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    #'tfidf__use_idf': (True, False),\n",
    "  #  'vectorizer_tfidf__max_n': (1, 2),\n",
    "   # 'vectorizer_tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vectorizer_tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    \n",
    "#                          n_estimators = 1000,\n",
    "#                         gamma=0.61, #2.7\n",
    "#                         min_child_weight=0.1,                         \n",
    "#                         learning_rate =0.3, #0,55 with bytree 1.... 0.3/0.38 best\n",
    "#                         reg_alpha=0,\n",
    "#                         max_depth=16,\n",
    "#                         #process_type='update',\n",
    "#                         #num_parallel_tree=3\n",
    "                        \n",
    "#                         colsample_bytree=1, colsample_bylevel=0.5, colsample_bynode=1\n",
    "    \n",
    "    \n",
    "      \n",
    "    #'xgb__max_cat_to_onehot': [4,8,12] #,2,4   \n",
    "   \n",
    "    #'xgb__learning_rate' : [0.081,0.082,0.083,0.084,0.085,0.086,0.087,0.088,0.089,0.09, 0.092], #0.05, 0.1 , 0.15, 0.2, 0.25, 0.3], |0.01, 0.03, 0.06, 0.08, 0.1, 0.15, 0.2, 0.25, 0.3\n",
    "    #',    \n",
    "    \n",
    "    #'xgb__eta': [0.1, 0.2,0.3],\n",
    "    #'xgb__scale_pos_weight':[0.1,0.2,0.3,0.4,0.5,0.6,0.7] -> lose 10%\n",
    "   #  'xgb__sampling_method':[gradient_based], ->no d\n",
    "    \n",
    "    #'xgb__subsample': [0.9], #nodif i/10.0 for i in range(6,10)\n",
    "    \n",
    "    \n",
    "   # 'xgb__n_estimators': [1000], #, 1500,2000\n",
    "  \n",
    "   \n",
    "   \n",
    "  # 'xgb__colsample_bylevel':[0.5], \n",
    "   # 'xgb__colsample_bynode':[1]\n",
    "  \n",
    "    #colsample_bytree=1, colsample_bylevel=0.5, colsample_bynode=1,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbf6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34e6206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up GridSearch\n",
    "# Setting up GridSearch for Randomforest\n",
    "gs = GridSearchCV(clf, params, \n",
    "                  cv = 5, \n",
    "                  verbose = 2,\n",
    "                  n_jobs = 1, \n",
    "                  scoring = 'f1',\n",
    "               #  scoring = [\"r2\", \"neg_root_mean_squared_error\"], #sklearn.metrics.SCORERS.keys() \n",
    "                                                 #scoring = ['accuracy', 'precision'],\n",
    "                #  refit = \"r2\"\n",
    "                 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b6d7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting Randomforest CV GS\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967f84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Best score: %0.3f' % gs.best_score_)\n",
    "print ('Best parameters set:')\n",
    "best_parameters = gs.best_estimator_.get_params()\n",
    "#      for param_name in sorted(parameters.keys():\n",
    "#          print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "\n",
    "#     predictions = gs.predict(X_test)\n",
    "#     print classification_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a618e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_estimator_) # to get the complete details of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f78090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gs.best_estimator_.steps)\n",
    "best_clf = gs.best_estimator_\n",
    "y_pred = best_clf.predict(X_test)\n",
    "\n",
    "#3. get the predictions for X_test and store it in y_pred\n",
    "#y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#4. print the classfication report\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"The accuracy of prediction is: \", accuracy)\n",
    "\n",
    "#df = pd.DataFrame(classification_report(y_test, y_pred))\n",
    "#df.to_csv(\"xgb_score.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6b443",
   "metadata": {},
   "outputs": [],
   "source": [
    " # simple filename parser and output results\n",
    "import numpy as np\n",
    "  #  p = np.vectorize(lambda x: x.split('.')[0].split('/')[2])\n",
    "   # util.write_predictions(y_pred, p(test.filenames).tolist(), 'tfidfvec_gs.csv')\n",
    "\n",
    " df = pd.DataFrame(gs.cv_results_)\n",
    " df = df.sort_values(\"rank_test_score\")\n",
    " df.to_csv(\"xgb_results.txt\", index=False)\n",
    "\n",
    "# df =(classification_report(y_test, y_pred))\n",
    "# df.to_csv(\"xgb_results.txt\", index=False)\n",
    "\n",
    "# cv_results = gs.cv_results_\n",
    "\n",
    "# report = classification_report(y_test, y_pred, output_dict=True)\n",
    "# print (report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab775d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test[:100]\n",
    "c =0\n",
    "for i in X_test[:200]:\n",
    "    print(c,\"->\", i)\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f75628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#y_test[0:50]\n",
    "c =0\n",
    "for i in y_test[0:]:\n",
    "    print(c,\"->\", i)\n",
    "    c+=1\n",
    "\n",
    "h=0\n",
    "nh=0\n",
    "for i in y_test:\n",
    "    if i == 1:\n",
    "        #print(i)\n",
    "        h+=1\n",
    "    if i == 0:\n",
    "        #print(i)\n",
    "        nh+=1\n",
    "print(\"count of HS: \", h) \n",
    "print(\"count of NS: \", nh)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f76fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred[0:50]\n",
    "c =0\n",
    "for i in y_pred[0:]:\n",
    "    print(c,\"->\", i)\n",
    "    c+=1\n",
    "\n",
    "h=0\n",
    "nh=0\n",
    "for i in y_pred:\n",
    "    if i == 1:\n",
    "        #print(i)\n",
    "        h+=1\n",
    "    if i == 0:\n",
    "        #print(i)\n",
    "        nh+=1\n",
    "print(\"count of HS: \", h) \n",
    "print(\"count of NS: \", nh)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090e98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96703ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
